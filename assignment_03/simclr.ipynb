{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-uirVvNW-yY"
   },
   "source": [
    "HHU Deep Representation Learning, Prof. Dr. Markus Kollmann\n",
    "\n",
    "Lecturers and Tutoring is done by Nikolas Adaloglou and Felix Michels.\n",
    "\n",
    "# Assignment 04 - Contrastive self-supervised learning: SimCLR in STL10 with Resnet18 \n",
    "---\n",
    "\n",
    "Submit the solved notebook (not a zip) with your full name plus assignment number for the filename as an indicator, e.g `max_mustermann_a1.ipynb` for assignment 1. If we feel like you have genuinely tried to solve the exercise, you will receive 1 point for this assignment, regardless of the quality of your solution.\n",
    "\n",
    "## <center> DUE FRIDAY 9.05.2024 2:30 pm </center>\n",
    "\n",
    "Drop-off link: [https://uni-duesseldorf.sciebo.de/s/XxYRWrv7uYuC92P](https://uni-duesseldorf.sciebo.de/s/XxYRWrv7uYuC92P)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Introduction \n",
    "\n",
    "Contrastive loss is a way of training a machine learning model in a self-supervised manner, where the goal is to learn meaningful representations of the input data without any explicit labels or annotations.\n",
    "\n",
    "The basic idea is to take a pair of input samples (such as two augmented views from the same image), and compare them to see if they are similar or dissimilar. The model is then trained to push similar pairs closer together in the representation space, while pushing dissimilar pairs farther apart.\n",
    "\n",
    "To do this, the contrastive loss function measures the similarity between the representations of the two input samples (nominator), and encourages the model to maximize this similarity if the samples are similar, and minimize it if they are dissimilar.\n",
    "\n",
    "\n",
    "You can also advice the [SimCLR Paper](https://arxiv.org/abs/2002.05709)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lw5K7r5SQDca"
   },
   "source": [
    "\n",
    "### Imports, basic utils, augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://raw.githubusercontent.com/HHU-MMBS/RepresentationLearning_PUBLIC_2024/main/exercises/week04/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1SIad0uuHBlv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import STL10\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "# Local imports\n",
    "from utils import save_model, load_model, linear_eval, \\\n",
    "    tsne_plot_embeddings, reproducibility, define_param_groups, \\\n",
    "    get_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Implement the augmentation pipeline used in SimCLR\n",
    "\n",
    "\n",
    "In contrastive self-supervised learning, there are several image augmentations that are commonly used to create pairs of images that are transformed versions of each other. These augmentations are designed to ensure that the resulting views have enough differences between them so that the model can learn to distinguish between them, while also preserving the label-related information.\n",
    "\n",
    "Some of the commonly used image augmentations in contrastive self-supervised learning include in **random order**:\n",
    "\n",
    "\n",
    "- Random flipping: This involves randomly flipping the image horizontally or vertically. Choose the one that best fits with a probability of 50%.\n",
    "- Normalize the images with an appropriate mean std.\n",
    "- Color jitter: This involves randomly changing the brightness, contrast, saturation and hue (20%) of the image. This augmentation helps the model learn to recognize objects or scenes under different lighting conditions. Apply this augmentation with a probability of 80%. Distort the brightness, contrast, saturation in the range `[0.2, 1.8]`.\n",
    "- Random cropping: This involves randomly cropping a portion of the image to create a new image. We will then resize the images to 64x64 instead of 96x96 to reduce the computational time complexity to train the model.  Use a scale of 10-100% of the initial image size. \n",
    "- Gaussian blur: This augmentation helps the model learn to recognize objects or scenes that are slightly out of focus. Use a `kernel_size` of 3 and Standard deviation of 0.1 to 2.0.\n",
    "\n",
    "\n",
    "The above augmentations are typically applied randomly to each image in a pair, resulting in two slightly different versions of the same image that can be used for contrastive learning.\n",
    "\n",
    "Your task is to define the augmentation and decide in which order they should be applied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augment:\n",
    "    \"\"\"\n",
    "    A stochastic data augmentation module\n",
    "    Transforms any given data example randomly\n",
    "    resulting in two correlated views of the same example,\n",
    "    denoted x ̃i and x ̃j, which we consider as a positive pair.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size):\n",
    "        ### START CODE HERE ###\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "def load_data( batch_size=128, train_split=\"unlabeled\", test_split=\"test\", transf = T.ToTensor()):\n",
    "    # Returns a train and validation dataloader for STL10 dataset\n",
    "    ### START CODE HERE ### (≈ 6 lines of code)\n",
    "    ### END CODE HERE ###\n",
    "    return train_dl, val_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4v-Qg5Xpk2Bv"
   },
   "source": [
    "## Task 2: Implement the SimCLR Contrastive loss (NT-Xent)\n",
    "\n",
    "Let $sim(u,v)$ note the dot product between 2 normalized $u$ and $v$ (i.e. cosine similarity). Then the loss function for a **positive pair**\n",
    "of examples (i,j) is defined as:\n",
    "$$\n",
    "\\ell_{i, j}=-\\log \\frac{\\exp \\left(\\operatorname{sim}\\left(\\boldsymbol{z}_{i}, \\boldsymbol{z}_{j}\\right) / \\tau\\right)}{\\sum_{k=1}^{2 N} \\mathbb{1}_{[k \\neq i]} \\exp \\left(\\operatorname{sim}\\left(\\boldsymbol{z}_{i}, \\boldsymbol{z}_{k}\\right) / \\tau\\right)}\n",
    "$$\n",
    "\n",
    "where $\\mathbb{1}_{[k \\neq i]} $ ∈{0,1} is an indicator function evaluating to 1 iff $k != i$ and τ denotes a temperature parameter. The final loss is computed by summing all positive pairs and divide by $2\\times N = views \\times batch_{size} $\n",
    "\n",
    "There are different ways to develop contrastive loss. \n",
    "\n",
    "\n",
    "#### Hints\n",
    "Here we provide you with some hints about the main algorithm:\n",
    "\n",
    "- apply l2 normalization to the features and concatenate them in the batch dimension\n",
    "\n",
    "- Calculate the similarity/logits of all pairs.  Output shape:[batch_size $\\times$ views,batch_size $\\times$ views]\n",
    "\n",
    "- Make Identity matrix as mask with size=(batch_size $\\times$ views, batch_size $\\times$ views)\n",
    "\n",
    "- Repeat the mask in both direction to the number of views (in simclr number of views = 2)\n",
    "for batch_size=5 and 2 views: \n",
    "```\n",
    "[1., 0., 0., 0., 0., 1., 0., 0., 0., 0.]\n",
    "[0., 1., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
    "[0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
    "[0., 0., 0., 1., 0., 0., 0., 0., 1., 0.],\n",
    "[0., 0., 0., 0., 1., 0., 0., 0., 0., 1.],\n",
    "[1., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
    "[0., 1., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
    "[0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
    "[0., 0., 0., 1., 0., 0., 0., 0., 1., 0.],\n",
    "[0., 0., 0., 0., 1., 0., 0., 0., 0., 1.]\n",
    "```\n",
    "\n",
    "4. Make a mask to index the positive pairs. mask-out the self-contrast as follows.\n",
    "make a mask with the shape of the logits = [batch_size $\\times$ views,batch_size $\\times$ views]  that has ones in the diagonals that are +- batch_size from the main diagonal. this will be used to index the positive pairs.\n",
    "Example for [6,6] matrix (batch_size=3,views=2):\n",
    "```\n",
    "[0., 0., 0., 1., 0., 0.],\n",
    "[0., 0., 0., 0., 1., 0.],\n",
    "[0., 0., 0., 0., 0., 1.],\n",
    "[1., 0., 0., 0., 0., 0.],\n",
    "[0., 1., 0., 0., 0., 0.],\n",
    "[0., 0., 1., 0., 0., 0.]\n",
    "``` \n",
    "Ones here will be the positive elements for the nominator.\n",
    "Alternativly you can use torch.diag() to take the positives from the  [6,6] similarity matrix (aka logits)\n",
    "\n",
    "- Use the positives to form the nominator.Scale down result with the temperature. There are batch_size $\\times$ views positive pairs.\n",
    "\n",
    "- Calculate the denominator by summing the masked logits in the correct dimension.\n",
    "\n",
    "- dont forget to apply `-log(result)`\n",
    "\n",
    "- Calculate the final loss as in the above equation.\n",
    "\n",
    "\n",
    "#### A note on L2 normalization\n",
    "\n",
    "L2 normalization is a common technique used in contrastive learning to normalize the embedding vectors before computing the contrastive loss. \n",
    "\n",
    "This is because L2 normalization scales the vectors to have unit length. Without L2 normalization, the magnitude of the embedding vectors can have a large influence on the contrastive loss. \n",
    "\n",
    "This can result in the optimization process focusing more on adjusting the magnitude of the vectors rather than their direction, leading to suboptimal solutions. \n",
    "\n",
    "By normalizing the embeddings, the contrastive loss only considers the angular difference between embedding vectors.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FeP4ZuZpsyOp"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Vanilla Contrastive loss, also called InfoNceLoss as in SimCLR paper\n",
    "    There are different ways to develop contrastive loss. Here we provide you with some hints about the main algorithm:\n",
    "        1- create an Identity matrix as a mask (bsz, bsz)\n",
    "        2- repeat the mask in both direction to the number of views (in simclr number of views = 2) in the above code we called it anchor_count\n",
    "        3- modify the mask to remove the self contrast cases\n",
    "        4- calculate the similarity of two features. *Note: final size should be  [bsz, bsz]\n",
    "        5- apply the mask on similairty matrix \n",
    "        6- calculate the final loss \n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "def test_ContrastiveLoss():\n",
    "    # Test the ContrastiveLoss\n",
    "    batch_size = 8\n",
    "    temperature = 0.1\n",
    "    criterion = ContrastiveLoss(batch_size, temperature)\n",
    "    proj_1 = torch.rand(batch_size, 128)\n",
    "    proj_2 = torch.rand(batch_size, 128)\n",
    "    loss = criterion(proj_1, proj_2)\n",
    "    assert loss.shape == torch.Size([]), \"ContrastiveLoss output shape is wrong\"\n",
    "    assert loss.item() >= 0, \"ContrastiveLoss output is negative\"\n",
    "    print(\"ContrastiveLoss test passed!\")\n",
    "\n",
    "test_ContrastiveLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8iM6b8CQjSy"
   },
   "source": [
    "# Task 3: Load and modify resnet18\n",
    "\n",
    "- Load and modify the resnet18.\n",
    "- Add an MLP with batch normalization after the resnet18 backbone as illustrate below:\n",
    "```python\n",
    "Sequential(\n",
    "  (0): Linear(in_features=in_features, out_features=in_features, bias=False)\n",
    "  (1): BatchNorm(in_features)\n",
    "  (2): ReLU()\n",
    "  (3): Linear(in_features=in_features, out_features=embedding_size, bias=False)\n",
    "  (4): BatchNorm(embedding_size))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WpEEBp7EH7-x"
   },
   "outputs": [],
   "source": [
    "class ResNetSimCLR(nn.Module):\n",
    "    def __init__(self, embedding_size=128):\n",
    "        super(ResNetSimCLR, self).__init__()\n",
    "        ### START CODE HERE ### (>10 lines of code)\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppxywhSH_Xjc"
   },
   "source": [
    "## Task 4: Gradient Accumulation: Implement the `training_step`  and `pretrain_one_epoch_grad_acc`\n",
    "\n",
    "- `training_step` should load a batch of 2 image views and feed them to the model. The loss function will calculate the implemented SimCLR loss.\n",
    "- Gradient accumulation saves the gradient values for a number of N steps. It calculates the gradients and proceeds to the next batch. Remember that when you call `loss.backward()` the newly computed gradients are added to the old ones. After N steps, the parameter update is done and the loss shall be scaled down (averaged) by the number of N iterations.\n",
    "\n",
    "Note: SimCLR training requires a large batch size. You should be to train SimCLR with a batch size of 512 on Google Colab.\n",
    "\n",
    "To reduce the required memory we use gradient accumulation and mixed precision training.\n",
    "\n",
    "#### Explanation of accumulated gradients\n",
    "\n",
    "When training large neural networks, the computational cost of computing the gradient for all of the training examples in the dataset can be prohibitive. Accumulated gradients is a technique used to increase the size of the batch of training samples used to update the weights of the network. \n",
    "\n",
    "Instead of applying the gradients to the model's parameters after each batch, the gradients are accumulated over a batch of training examples. The accumulated gradients are then used to update the model's parameters. In this way, one reduces the noise in the gradients by averaging them over a batch of training examples, which can lead to more stable updates to the model's parameters. It also allows the model to make larger updates to its parameters, which may speed up the training process.\n",
    "\n",
    "For supervised training, the accumulated gradients are exactly the same as they would be for a larger batch size. This is not the case for the contrastive loss!\n",
    "\n",
    "For example, if we set the batch size to 32, the network would process 32 examples at a time, compute the gradients for each example, and then accumulate the gradients over the 32 examples. After accumulating the gradients for the entire batch, the weights of the network are updated using the average of the accumulated gradients. Thus, for a batch size of 32 you can accumulate gradients every N steps so that you have an effective batch size of 32 $\\times$ N!\n",
    "\n",
    "> Importantly, gradient accumulation slows down training since gradient updates happen every N steps, but it is expected to see the loss dropping steadily and probably faster, depending on the method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5ukADmI_d_H"
   },
   "outputs": [],
   "source": [
    "def pretrain_one_epoch_grad_acc(model, loss_function, train_dataloader, optimizer, accum_iter,\n",
    "                               device, dtype, scaler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = len(train_dataloader)\n",
    "    optimizer.zero_grad()\n",
    "    for batch_idx, ((view1, view2), _) in enumerate(tqdm(train_dataloader, leave=False)):\n",
    "        with torch.autocast(device_type=device.type, dtype=dtype):\n",
    "            view1 = view1.to(device)\n",
    "            view2 = view2.to(device)\n",
    "            loss = training_step(model, loss_function, view1, view2)\n",
    "            # normalize loss to account for batch accumulation\n",
    "            loss = loss / accum_iter\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        # weights update\n",
    "        if ((batch_idx + 1) % accum_iter == 0) or (batch_idx + 1 == num_batches):\n",
    "            if scaler is not None:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        total_loss += loss.item()*accum_iter\n",
    "    \n",
    "    return total_loss/num_batches\n",
    "\n",
    "\n",
    "def training_step(model, loss_function, view1, view2):\n",
    "    ### START CODE HERE ### (≈ 5 lines of code)\n",
    "    ### END CODE HERE ###\n",
    "    return loss\n",
    "\n",
    "\n",
    "def pretrain(model, optimizer, num_epochs, train_loader, criterion, device, accum_iter=1):\n",
    "    dict_log = {\"train_loss\":[]}\n",
    "    device = torch.device(device)\n",
    "    dtype = torch.bfloat16 if (device.type == 'cpu' or torch.cuda.is_bf16_supported()) else torch.float16\n",
    "    scaler = torch.cuda.amp.GradScaler() if dtype == torch.float16 else None\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 12 lines of code)\n",
    "        ### END CODE HERE ### \n",
    "    return dict_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqNuy5R2AThH"
   },
   "source": [
    "# Task 5: Putting everything together and train the model\n",
    "\n",
    "Hint: ~50 epochs should be sufficient to see the learned features.\n",
    "\n",
    "A small training trick here. We will exclude batch normalization parameters from weight decay in `define_param_groups`\n",
    "\n",
    "Note on complexity: 10.7 VRAM used and ~156mins needed. Effective batch size=2048, images of 64x64, 60 epochs.\n",
    "As always, feel free to train less epochs, depending on the available resources.\n",
    "\n",
    "In case you face problem with Google colab, download the model every 5 epochs or better mount you google drive and save the model there in case you disconnect.\n",
    "\n",
    "Here\n",
    "```python\n",
    "PATH = './best_model.ckpt'\n",
    "torch.save(model_simclr.state_dict(), PATH)\n",
    "files.download(PATH)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 77777\n",
    "reproducibility(seed)\n",
    "\n",
    "# Set hyperparameters\n",
    "### START CODE HERE ### (>10 lines of code)\n",
    "### END CODE HERE ###\n",
    "\n",
    "config = Hparams()\n",
    "model = ResNetSimCLR(embedding_size=config.embedding_size)\n",
    "criterion = ContrastiveLoss(batch_size=config.batch_size, temperature=config.temperature)\n",
    "train_dl , _ = load_data(batch_size=config.batch_size, train_split=\"unlabeled\", test_split=\"test\", transf=Augment(64))\n",
    "param_groups = define_param_groups(model, config.weight_decay, 'adam')\n",
    "optimizer = Adam(param_groups, lr=config.lr, weight_decay=config.weight_decay)\n",
    "\n",
    "# Launch training!\n",
    "dict_log = pretrain(model, optimizer, config.epochs, train_dl, criterion, config.device, accum_iter=config.accum_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6:Linear probing + T-SNE visualization of features\n",
    "\n",
    "As in the previous exercise, check the results of linear probing on the supervised training split and the T-SNE visualization. Remember not to use the projection head.\n",
    "\n",
    "Code for the T-SNE visualization exists in `utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (>10 line of code)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected results\n",
    "Final loss ~5.5\n",
    "\n",
    "![](https://raw.githubusercontent.com/HHU-MMBS/RepresentationLearning_PUBLIC_2024/main/exercises/week04/figs/tsne_plot_embeddings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FrwRzDnAst5"
   },
   "source": [
    "# Task 7: Fine-tune on downstream task: supervised image classification on STL10 train split\n",
    "\n",
    "Variants to be tested: \n",
    "- SimCLR weights trained for at least 50 epochs\n",
    "- Imagenet initialization\n",
    "- random initialization\n",
    "\n",
    "Afterwards, print the best val. accuracy for all 3 models!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(mode='simclr'):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    ### START CODE HERE ### (>10 lines of code)\n",
    "    ### END CODE HERE ###\n",
    "    dict_log = linear_eval(model, optimizer, 10, train_dl, val_dl, device)\n",
    "    return dict_log\n",
    "\n",
    "dict_log_simclr = finetune('simclr')\n",
    "acc1 = np.max(dict_log_simclr[\"val_acc_epoch\"])\n",
    "dict_log_in = finetune('imagenet')\n",
    "acc2 = np.max(dict_log_in[\"val_acc_epoch\"])\n",
    "dict_log_ran = finetune('random')\n",
    "acc3 = np.max(dict_log_ran[\"val_acc_epoch\"])\n",
    "print(f\"Fine-tuning best results: SimCLR: {acc1:.2f}%, ImageNet: {acc2:.2f} %, Random: {acc3:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPqA2qOp9vl6"
   },
   "source": [
    "### Expected results\n",
    "\n",
    "By fine-tuning all variants for 20 epochs this is what we got: \n",
    "\n",
    "\n",
    "```\n",
    "Fine-tuning best results: SimCLR: 67.87%, ImageNet: 74.21 %, Random: 48.94 %\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8:Plot the val accuracies for the 3 different initializations\n",
    "\n",
    "Plot is left intentionally to cross-check you results qualitatively\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(dict_log_simclr[\"val_acc_epoch\"], label=\"SimCLR\")\n",
    "plt.plot(dict_log_in[\"val_acc_epoch\"], label=\"ImageNet\")\n",
    "plt.plot(dict_log_ran[\"val_acc_epoch\"], label=\"Random\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Fine tuning results on STL-10\")\n",
    "plt.savefig(\"fine_tuning_results_stl10.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected result\n",
    "![](https://raw.githubusercontent.com/HHU-MMBS/RepresentationLearning_PUBLIC_2024/main/exercises/week04/figs/fine_tuning_results_stl10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Bonus reads\n",
    "\n",
    "That's the end of this exercise. If you reached this point, congratulations!\n",
    "\n",
    "\n",
    "### Optional stuff\n",
    "\n",
    "- Improve SimCLR. Add the [LARS optimizer](https://gist.github.com/black0017/3766fc7c62bdd274df664f8ec03715a2) with linear warm + [cosine scheduler](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html?highlight=cosine%20scheduler#torch.optim.lr_scheduler.CosineAnnealingLR) + train for 200 epochs. Then make a new comparison!\n",
    "- Train on CIFAR100 and compare rotation prediction VS SimCLR pretraining on both datasets. Which pretext task is likely to work better there?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "[Exercise 4] - SimCLR Resnet18 Solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc5fcf396fe0abd4fa852aee332a0572494dcaf5776820055c87d9b84157f362"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
