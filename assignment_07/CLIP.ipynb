{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCfZVwhwMIYN"
   },
   "source": [
    "HHU Deep Representation Learning, Prof. Dr. Markus Kollmann\n",
    "\n",
    "Lecturers and Tutoring is done by Nikolas Adaloglou and Felix Michels.\n",
    "\n",
    "# Assignment 07 - CLIP for unsupervised OOD detection (2-week exercise)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Submit the solved notebook (not a zip) with your full name plus assingment number for the filename as an indicator, e.g `max_mustermann_a1.ipynb` for assignment 1.\n",
    "\n",
    "This is a **two week exercise**. If we feel like you have genuinely tried to solve the exercise, you will receive **2** points for this assignment, regardless of the quality of your solution.\n",
    "\n",
    "## <center> DUE FRIDAY 27.06.2025 </center>\n",
    "\n",
    "Drop-off link: [https://uni-duesseldorf.sciebo.de/s/QUtbyMFxxKPtCWO](https://uni-duesseldorf.sciebo.de/s/QUtbyMFxxKPtCWO)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. Basic imports\n",
    "2. Get the visual features of the CLIP model\n",
    "3. Compute the k-NN similarity as the OOD score\n",
    "4. Compute MSP using the text encoder and the label names\n",
    "5. Linear probing on the pseudolabels\n",
    "6. Mahalanobis distance as OOD score\n",
    "7. Mahalanobis distance using the real labels without linear probing\n",
    "8. K-means clusters combined with Mahalanobis distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Overview\n",
    "We will apply the learned representations from Contrastive Language-Image Pretrained (CLIP) on the downstream task of out-of-distribution detection.\n",
    "\n",
    "We will be using the model 'convnext_base_w' pretrained on 'laion2b_s13b_b82k' throughout this tutorial.\n",
    "\n",
    "Info and examples on how to use CLIP models for inference is provided in [openclip](https://github.com/mlfoundations/open_clip#usage)\n",
    "\n",
    "- [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)\n",
    "- [Contrastive Language-Image Pretrained (CLIP) Models are Powerful Out-of-Distribution Detectors](https://arxiv.org/abs/2303.05828)\n",
    "\n",
    "\n",
    "\n",
    "# Part I. Basic imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T23:58:52.470351Z",
     "start_time": "2025-06-26T23:58:52.293093Z"
    }
   },
   "source": "!wget -nc https://raw.githubusercontent.com/HHU-MMBS/RepresentationLearning_PUBLIC_2024/main/exercises/week08/utils.py",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘utils.py’ already there; not retrieving.\r\n",
      "\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T23:58:53.425302Z",
     "start_time": "2025-06-26T23:58:52.594251Z"
    }
   },
   "source": "!pip install open_clip_torch timm",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: open_clip_torch in /opt/homebrew/anaconda3/envs/representation-learning/lib/python3.13/site-packages (2.32.0)\r\n",
      "Requirement already satisfied: timm in /opt/homebrew/anaconda3/envs/representation-learning/lib/python3.13/site-packages (1.0.15)\r\n",
      "Requirement already satisfied: torch>=1.9.0 in /opt/homebrew/anaconda3/envs/representation-learning/lib/python3.13/site-packages (from open_clip_torch) (2.6.0)\r\n",
      "Requirement already satisfied: torchvision in /opt/homebrew/anaconda3/envs/representation-learning/lib/python3.13/site-packages (from open_clip_torch) (0.21.0)\r\n",
      "Requirement already satisfied: regex in /opt/homebrew/anaconda3/envs/representation-learning/lib/python3.13/site-packages (from open_clip_torch) (2024.11.6)\r\n",
      "Requirement already satisfied: ftfy in /opt/homebrew/anaconda3/envs/representation-learning/lib/python3.13/site-packages (from open_clip_torch) (6.3.1)\r\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/anaconda3/envs/representation-learning/lib/python3.13/site-packages (from open_clip_torch) (4.67.1)\r\n",
      "Requirement already satisfied: huggingface-hub in /opt/homebrew/anaconda3/envs/representation-learning/lib/python3.13/site-packages (from open_clip_torch) (0.30.2)\r\n",
      "Requirement already satisfied: safetensors in /opt/homebrew/anaconda3/envs/representation-learning/lib/python3.13/site-packages (from open_clip_torch) (0.5.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/homebrew/anaconda3/envs/representation-learning/lib/python3.13/site-packages (from timm) (6.0.2)\r\n",
      "Requirement already satisfied: filelock in /opt/homebrew/anaconda3/envs/representation-learning/lib/python3.13/site-packages (from torch>=1.9.0->open_clip_torch) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/homebrew/anaconda3/envs/representation-learning/lib/python3.13/site-packages (from torch>=1.9.0->open_clip_torch) (4.13.2)\r\n",
      "Requirement already satisfied: networkx in /opt/homebrew/anaconda3/envs/representation-learning/lib/python3.13/site-packages (from torch>=1.9.0->open_clip_torch) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/anaconda3/envs/representation-learning/lib/python3.13/site-packages (from torch>=1.9.0->open_clip_torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/anaconda3/envs/representation-learning/lib/python3.13/site-packages (from torch>=1.9.0->open_clip_torch) (2024.12.0)\r\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/anaconda3/envs/representation-learning/lib/python3.13/site-packages (from torch>=1.9.0->open_clip_torch) (75.8.2)\r\n",
      "Requirement already satisfied: sympy!=1.13.2,>=1.13.1 in /opt/homebrew/anaconda3/envs/representation-learning/lib/python3.13/site-packages (from torch>=1.9.0->open_clip_torch) (1.13.3)\r\n",
      "Requirement already satisfied: wcwidth in /opt/homebrew/anaconda3/envs/representation-learning/lib/python3.13/site-packages (from ftfy->open_clip_torch) (0.2.13)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/homebrew/anaconda3/envs/representation-learning/lib/python3.13/site-packages (from huggingface-hub->open_clip_torch) (24.2)\r\n",
      "Requirement already satisfied: requests in /opt/homebrew/anaconda3/envs/representation-learning/lib/python3.13/site-packages (from huggingface-hub->open_clip_torch) (2.32.3)\r\n",
      "Requirement already satisfied: numpy in /opt/homebrew/anaconda3/envs/representation-learning/lib/python3.13/site-packages (from torchvision->open_clip_torch) (2.2.5)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/homebrew/anaconda3/envs/representation-learning/lib/python3.13/site-packages (from torchvision->open_clip_torch) (11.1.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/anaconda3/envs/representation-learning/lib/python3.13/site-packages (from sympy!=1.13.2,>=1.13.1->torch>=1.9.0->open_clip_torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/anaconda3/envs/representation-learning/lib/python3.13/site-packages (from jinja2->torch>=1.9.0->open_clip_torch) (3.0.2)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/anaconda3/envs/representation-learning/lib/python3.13/site-packages (from requests->huggingface-hub->open_clip_torch) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/anaconda3/envs/representation-learning/lib/python3.13/site-packages (from requests->huggingface-hub->open_clip_torch) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/anaconda3/envs/representation-learning/lib/python3.13/site-packages (from requests->huggingface-hub->open_clip_torch) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/anaconda3/envs/representation-learning/lib/python3.13/site-packages (from requests->huggingface-hub->open_clip_torch) (2025.6.15)\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T23:58:53.441267Z",
     "start_time": "2025-06-26T23:58:53.436563Z"
    }
   },
   "cell_type": "code",
   "source": "LOAD_DATA = True",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T23:58:58.887072Z",
     "start_time": "2025-06-26T23:58:53.452258Z"
    }
   },
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import open_clip\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from sklearn.cluster import KMeans\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.auto import trange\n",
    "\n",
    "models_dir = Path('./models/').resolve()\n",
    "feats_dir = Path('./features/').resolve()\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "feats_dir.mkdir(exist_ok=True)\n",
    "# Local import\n",
    "from utils import get_features, auroc_score, linear_eval, load_model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu')"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Get the visual features of the CLIP model\n",
    "\n",
    "- We will use `CIFAR100` as the in-distribution, and `CIFAR10` as the out-distribution.\n",
    "- When you are only loading the visual CLIP backbone, you must remove the final linear layer that projects the features to the shared feature space of the image-text encoder.\n",
    "- Load the data, compute the visual features and save them in the `features` folder.\n",
    "- For the in-distribution you need both the train and test split, while for the out-distribution, we will only use the validation split.\n",
    "\n",
    "\n",
    "### Optional structure\n",
    "\n",
    "```python\n",
    "def load_datasets(indist=\"CIFAR100\", ood=\"CIFAR10\", batch_size=256, tranform=None):\n",
    "    # ....\n",
    "    return indist_train_loader, indist_test_loader, ood_loader\n",
    "\n",
    "# visual is a boolean that controls whether the visual backbone is only returned or the whole CLIP model\n",
    "def get_model(visual, name, pretrained)\n",
    "    # .....\n",
    "    if visual:\n",
    "        # ....\n",
    "        return backbone, preprocess\n",
    "    return model, preprocess, tokenizer\n",
    "    \n",
    "# Load everything .......\n",
    "\n",
    "feats, labels = get_features(backbone, dl, device)\n",
    "# Save features \n",
    "# ....\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T23:58:58.900486Z",
     "start_time": "2025-06-26T23:58:58.896670Z"
    }
   },
   "source": [
    "### START CODE HERE ### (≈ 30 lines of code)\n",
    "def load_data(batch_size=256, indist='CIFAR100', ood='CIFAR10', transform=T.ToTensor()):\n",
    "    ds = lambda name, train: getattr(torchvision.datasets, name)(root='~/data', train=train, download=True, transform=transform)\n",
    "    dl = lambda name, train: DataLoader(ds(name, train), batch_size=batch_size, shuffle=train)\n",
    "\n",
    "    return dl(indist, train=True), dl(indist, train=False), dl(ood, train=False)\n",
    "\n",
    "\n",
    "def get_model(visual=False, name='convnext_base_w', pretrained='laion2b_s13b_b82k'):\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(name, pretrained, device=device)\n",
    "    if not visual: return model, preprocess, open_clip.get_tokenizer(name)\n",
    "\n",
    "    backbone = model.visual\n",
    "    backbone.head = nn.Identity()\n",
    "    return backbone, preprocess"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T23:59:03.551124Z",
     "start_time": "2025-06-26T23:58:58.911023Z"
    }
   },
   "cell_type": "code",
   "source": "model, _ = get_model(True)",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T23:59:09.500502Z",
     "start_time": "2025-06-26T23:59:03.577797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filenames = ['cifar100_train', 'cifar100_test', 'cifar10_test']\n",
    "backbone, preprocess = get_model(True)\n",
    "dls = load_data(transform=preprocess)\n",
    "for name, dl in zip(filenames, dls):\n",
    "    if LOAD_DATA and os.path.exists(feats_dir / f'{name}_feats.pt'): continue\n",
    "\n",
    "    feats, labels = get_features(backbone, dl, device)\n",
    "    torch.save(feats, feats_dir / f'{name}_feats.pt')\n",
    "    torch.save(labels, feats_dir / f'{name}_labels.pt')\n",
    "### END CODE HERE ###"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T23:59:09.704124Z",
     "start_time": "2025-06-26T23:59:09.514026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# feature test\n",
    "for name, N in [('cifar100_train', 50000), ('cifar100_test', 10000), ('cifar10_test', 10000)]:\n",
    "    feats = torch.load(feats_dir / f'{name}_feats.pt')\n",
    "    labels = torch.load(feats_dir / f'{name}_labels.pt')\n",
    "    assert feats.shape == (N, 1024)\n",
    "    assert labels.shape == (N,)\n",
    "print('Success!')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III. Compute the k-NN similarity as the OOD score\n",
    "\n",
    "- For each test image of in and out distribution compute the top-1 cosine similarity and use it as OOD score.\n",
    "- Report the resulting AUROC score.\n",
    "- Note: Use the image features and not the images!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T23:59:17.920888Z",
     "start_time": "2025-06-26T23:59:09.775681Z"
    }
   },
   "source": [
    "@torch.no_grad()\n",
    "def OOD_classifier_knn(train_features, test_features, k=1, batch_size=1024):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        train_features: Tensor of shape NxD\n",
    "        test_features: Tensor of shape KxD\n",
    "        k: Number of closest train features to consider.\n",
    "           Optional for this exercise\n",
    "\n",
    "    Returns:\n",
    "        cos_sim: Top-k cosine similarity, tensor of shape K\n",
    "\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (≈ 13 lines of code)\n",
    "    B = batch_size\n",
    "    K, _ = test_features.shape\n",
    "\n",
    "    train_features = F.normalize(train_features, dim=-1)\n",
    "    test_features = F.normalize(test_features, dim=-1)\n",
    "\n",
    "    cos_sim = torch.empty(K, k, device=test_features.device, dtype=test_features.dtype)\n",
    "    for start in trange(0, K, B):\n",
    "        feat = test_features[start:start + B]\n",
    "        sim = feat @ train_features.T\n",
    "        cos_sim[start:start + B, :] = sim.topk(k)[0]\n",
    "    ### END CODE HERE ###\n",
    "    return cos_sim\n",
    "\n",
    "\n",
    "# load the computed features and compute scores\n",
    "### START CODE HERE ### (≈ 5 lines of code)\n",
    "cifar100_train_feats = torch.load(feats_dir / 'cifar100_train_feats.pt').to(device)\n",
    "cifar100_test_feats = torch.load(feats_dir / 'cifar100_test_feats.pt').to(device)\n",
    "cifar10_test_feats = torch.load(feats_dir / 'cifar10_test_feats.pt').to(device)\n",
    "score_in = OOD_classifier_knn(cifar100_train_feats, cifar100_test_feats).cpu()\n",
    "score_out = OOD_classifier_knn(cifar100_train_feats, cifar10_test_feats).cpu()\n",
    "### END CODE HERE ###\n",
    "print(f'CIFAR100-->CIFAR10 AUROC: {auroc_score(score_in, score_out):.2f}')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ace673fda1064786a5adfcdf1ec40c02"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "02b3baa1ce9948ae9307199c96570364"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR100-->CIFAR10 AUROC: 83.55\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected result\n",
    "\n",
    "```\n",
    "CIFAR100-->CIFAR10 AUROC: 83.55\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV. Compute MSP using the text encoder and the label names\n",
    "\n",
    "We will now consider the case where the in-distribution label names are available.\n",
    "\n",
    "Your task is to apply zero-shot classification and get the maximum softmax probability (MSP) as the OOD score.\n",
    "\n",
    "In short:\n",
    "- compute image and text embeddings\n",
    "- compute the image-test similarity matrix (logits)\n",
    "- apply softmax to the logits for each image to get a probability distribution of the classes.\n",
    "- compute maximum softmax probability (MSP)\n",
    "\n",
    "- `Note`: After loading the saved image features you need to apply the linear projection layer from the visual backbone of CLIP"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T23:59:18.990974Z",
     "start_time": "2025-06-26T23:59:18.987195Z"
    }
   },
   "source": [
    "@torch.inference_mode()\n",
    "def compute_msp(model, class_tokens, indist_test, ood_test, device):\n",
    "    ### START CODE HERE ### (≈ 7 lines of code)\n",
    "    scale = model.logit_scale.exp().item()\n",
    "\n",
    "    def score(img_emb, text_emb):\n",
    "        img_emb = F.normalize(img_emb, dim=-1)\n",
    "        text_emb = F.normalize(text_emb, dim=-1)\n",
    "        logits = (img_emb @ text_emb.T) * scale\n",
    "        probs = logits.softmax(dim=-1)\n",
    "        return probs.max(dim=-1).values.cpu()\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    class_tokens_emb = model.encode_text(class_tokens)\n",
    "\n",
    "    indist_test = indist_test.to(device)\n",
    "    ood_test = ood_test.to(device)\n",
    "\n",
    "    score_in = score(indist_test, class_tokens_emb)\n",
    "    score_out = score(ood_test, class_tokens_emb)\n",
    "    ### END CODE HERE ###\n",
    "    return score_in, score_out"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T23:59:23.349571Z",
     "start_time": "2025-06-26T23:59:19.044736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load model and features\n",
    "### START CODE HERE ### (≈ 4 lines of code)\n",
    "model, _, tokenizer = get_model()\n",
    "with torch.inference_mode():\n",
    "    indist_train = model.visual.head(cifar100_train_feats.to(device))\n",
    "    indist_test = model.visual.head(cifar100_test_feats.to(device))\n",
    "    ood_test = model.visual.head(cifar10_test_feats.to(device))\n",
    "### END CODE HERE ###"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T23:59:26.448158Z",
     "start_time": "2025-06-26T23:59:23.510865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# model, tokenizer,indist_test, ood_test = ...... need to be defined above\n",
    "### Provided\n",
    "label_names = torchvision.datasets.CIFAR100(root='~/data', train=True, download=True).classes\n",
    "prompts = ['an image of a ' + lab.replace('_', ' ') for lab in label_names]\n",
    "class_tokens = tokenizer(label_names).to(device)\n",
    "score_in, score_out = compute_msp(model, class_tokens, indist_test, ood_test, device)\n",
    "print(f'CIFAR100-->CIFAR10 AUROC: {auroc_score(score_in, score_out):.2f}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR100-->CIFAR10 AUROC: 76.38\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected result\n",
    "\n",
    "```\n",
    "CIFAR100-->CIFAR10 AUROC: 76.36\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V. Linear probing on the pseudolabels\n",
    "\n",
    "- Your task is to train a linear layer on the CLIP embeddings using the CLIP pseudolabels as targets.\n",
    "- The pseudolabels are the argmax of the logits computed above, i.e., take the class with the maximum probability as the class label"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T23:59:26.577089Z",
     "start_time": "2025-06-26T23:59:26.573473Z"
    }
   },
   "source": [
    "@torch.inference_mode()\n",
    "def compute_score_probe_msp(lin_layer, indist_dl, ood_dl, device):\n",
    "    \"\"\"\n",
    "    Computes the MSP scores for a linear layer for both in- and out- distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    # dataset small enough to run in single pass\n",
    "    score = lambda dl: lin_layer(dl.dataset.tensors[0]).softmax(dim=-1).max(dim=-1).values.cpu()\n",
    "\n",
    "    lin_layer.eval()\n",
    "    lin_layer.to(device)\n",
    "    score_in, score_out = score(indist_dl), score(ood_dl)\n",
    "    ### END CODE HERE ###\n",
    "    return score_in, score_out"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T23:59:31.930652Z",
     "start_time": "2025-06-26T23:59:26.712547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### START CODE HERE ### (≈ 17 lines of code)\n",
    "@torch.inference_mode()\n",
    "def load_pseudolabel_data(batch_size=256):\n",
    "    model, _, tokenizer = get_model()\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    scale = model.logit_scale.exp().item()\n",
    "\n",
    "    class_tokens_enc = model.encode_text(class_tokens)\n",
    "    text_emb = F.normalize(class_tokens_enc, dim=-1)\n",
    "\n",
    "    def calc_pseudolabel(img_emb):\n",
    "        img_emb = F.normalize(img_emb, dim=-1)\n",
    "        logits = (img_emb @ text_emb.T) * scale\n",
    "        probs = logits.softmax(dim=-1)\n",
    "        return probs.max(dim=-1).indices.cpu()\n",
    "\n",
    "    train_ds = TensorDataset(indist_train, calc_pseudolabel(indist_train))\n",
    "    val_ds = TensorDataset(indist_test, calc_pseudolabel(indist_test))\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_dl = DataLoader(val_ds, batch_size=batch_size)\n",
    "\n",
    "    return train_ds, val_ds, train_dl, val_dl\n",
    "\n",
    "\n",
    "train_ds, val_ds, train_dl, val_dl, = load_pseudolabel_data()\n",
    "### END CODE HERE ###"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T23:59:55.514031Z",
     "start_time": "2025-06-26T23:59:32.049971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The code below is provided based on our implementation. Optional to use!\n",
    "# Run linear probing\n",
    "embed_dim = train_ds[0][0].size(0)\n",
    "lin_layer = nn.Linear(embed_dim, 100).to(device)\n",
    "optimizer = torch.optim.Adam(lin_layer.parameters(), lr=1e-3)\n",
    "num_epochs = 20\n",
    "dict_log = linear_eval(lin_layer, optimizer, num_epochs, train_dl, val_dl, device, prefix=models_dir / 'CLIP')\n",
    "# compute MSP scores\n",
    "lin_layer = load_model(models_dir / \"CLIP_best_max_train_acc.pth\", device)\n",
    "ood_ds = TensorDataset(ood_test, torch.zeros(ood_test.shape[0], dtype=torch.long))\n",
    "ood_dl = DataLoader(ood_ds, batch_size=128, shuffle=False, drop_last=False)\n",
    "score_in, score_out = compute_score_probe_msp(lin_layer, val_dl, ood_dl, device)\n",
    "print(f'CIFAR100-->CIFAR10 AUROC: {auroc_score(score_in, score_out):.2f}')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "559e9349f6784bcaa5e556dee62aeb26"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model /Users/Mac/Documents/Uni/Representation Learning/assignment_07/models/CLIP_best_max_train_acc.pth is loaded from epoch 19\n",
      "CIFAR100-->CIFAR10 AUROC: 70.14\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected results\n",
    "\n",
    "AUROC may slightly vary due to random initialization of linear probing.\n",
    "```\n",
    "CIFAR100-->CIFAR10 AUROC: 74.81\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part VI. Mahalanobis distance as OOD score\n",
    "\n",
    "For definitions of the (relative) Mahalanobis distance, see [A Simple Fix to Mahalanobis Distance for Improving Near-OOD Detection](https://arxiv.org/abs/2106.09022)\n",
    "- Use the output of the linear layer from task 4 as features to compute the Mahalanobis distance and the relative Mahalanobis distance.\n",
    "- To compute the Mahalanobis distance group the features by their pseudolabels and compute the mean and covariance matrix for each class.\n",
    "- Only consider classes with at least two samples"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T23:59:55.689292Z",
     "start_time": "2025-06-26T23:59:55.685639Z"
    }
   },
   "cell_type": "code",
   "source": "torch.Tensor.cov",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<method 'cov' of 'torch._C.TensorBase' objects>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T23:59:57.756640Z",
     "start_time": "2025-06-26T23:59:55.811166Z"
    }
   },
   "source": [
    "@torch.inference_mode()\n",
    "def OOD_classifier_maha(train_embeds_in, train_labels_in, test_embeds_in, test_embeds_outs, num_classes,\n",
    "    relative=False):\n",
    "    ### START CODE HERE ### (≈ 23 lines of code)\n",
    "    labels = train_labels_in.unique().tolist()\n",
    "\n",
    "    # i'm not sure if this is correct as it boosted all results a lot\n",
    "    # but without it i sometimes get a auroc of 20-30, which doesn't make any sense...\n",
    "    cov_stabilizer = 1e-5 * torch.eye(train_embeds_in.shape[1], device=train_embeds_in.device)\n",
    "\n",
    "    x = {c: train_embeds_in[train_labels_in == c] for c in labels if (train_labels_in == c).sum() >= 2}\n",
    "\n",
    "    mu = {c: v.mean(dim=0) for c, v in x.items()}\n",
    "    inv_cov = {c: (v.T.cov() + cov_stabilizer).inverse() for c, v in x.items()}\n",
    "\n",
    "    if relative:\n",
    "        global_mu = train_embeds_in.mean(dim=0)\n",
    "        global_inv_cov = (train_embeds_in.T.cov() + cov_stabilizer).inverse()\n",
    "\n",
    "    def maha(x, mu_c, inv_cov_c):\n",
    "        centered = x - mu_c\n",
    "        return torch.einsum('bi,ij,bj->b', centered, inv_cov_c, centered)\n",
    "\n",
    "    def score(x_test):\n",
    "        dist = torch.stack([maha(x_test, mu[c], inv_cov[c]) for c in x]).min(dim=0).values\n",
    "        return dist - maha(x_test, global_mu, global_inv_cov) if relative else dist\n",
    "\n",
    "    scores_in = -score(test_embeds_in)\n",
    "    scores_out = -score(test_embeds_outs)\n",
    "    ### END CODE HERE ###\n",
    "    return scores_in, scores_out\n",
    "\n",
    "\n",
    "# The code below is provided based on our implementation. Optional to use!\n",
    "num_classes = 100\n",
    "lin_layer = load_model(models_dir / \"CLIP_best_max_train_acc.pth\", device)\n",
    "logits_indist_train, indist_pseudolabels_train = get_features(lin_layer, train_dl, device)\n",
    "logits_indist_test, indist_pseudolabels_test = get_features(lin_layer, val_dl, device)\n",
    "logits_ood, _ = get_features(lin_layer, ood_dl, device)\n",
    "\n",
    "# run OOD classifier based on mahalanobis distance\n",
    "scores_in, scores_out = OOD_classifier_maha(logits_indist_train, indist_pseudolabels_train,\n",
    "                                            logits_indist_test, logits_ood, num_classes, relative=False)\n",
    "print(f'Maha: CIFAR100-->CIFAR10 AUROC: {auroc_score(scores_in, scores_out):.2f}')\n",
    "scores_in, scores_out = OOD_classifier_maha(logits_indist_train, indist_pseudolabels_train,\n",
    "                                            logits_indist_test, logits_ood, num_classes, relative=True)\n",
    "print(f'Relative Maha: CIFAR100-->CIFAR10 AUROC: {auroc_score(scores_in, scores_out):.2f}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model /Users/Mac/Documents/Uni/Representation Learning/assignment_07/models/CLIP_best_max_train_acc.pth is loaded from epoch 19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e5c77febb524b9c8b9720c8b584e1c2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f2667595b9894d96b81979db1f044a0a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "75d52cec87c44da291852ceb06dd6c74"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maha: CIFAR100-->CIFAR10 AUROC: 81.80\n",
      "Relative Maha: CIFAR100-->CIFAR10 AUROC: 83.22\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected results\n",
    "(can differ based on linear probing performance)\n",
    "\n",
    "```\n",
    "Maha: CIFAR100-->CIFAR10 AUROC: 84.31\n",
    "Relative Maha: CIFAR100-->CIFAR10 AUROC: 88.20\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part VII. Mahalanobis distance using the real labels without linear probing\n",
    "- Again, compute the (relative) Mahalanobis distance as OOD score\n",
    "- This time, instead of using the pseudolabels and output of the linear probing layer, use the real labels of the training data and the features computed in task 1"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T00:00:05.421888Z",
     "start_time": "2025-06-26T23:59:57.893964Z"
    }
   },
   "source": [
    "### START CODE HERE ### (≈ 7 lines of code)\n",
    "indist_labels = torch.load(feats_dir / 'cifar100_train_labels.pt')\n",
    "scores_md_in, scores_md_out = OOD_classifier_maha(indist_train, indist_labels, indist_test, ood_test, num_classes, relative=True)\n",
    "scores_rmd_in, scores_rmd_out = OOD_classifier_maha(indist_train, indist_labels, indist_test, ood_test, num_classes, relative=False)\n",
    "### END CODE HERE ###\n",
    "print(f'Maha: CIFAR100-->CIFAR10 AUROC: {auroc_score(scores_md_in, scores_md_out):.2f}')\n",
    "print(f'Relative Maha: CIFAR100-->CIFAR10 AUROC: {auroc_score(scores_rmd_in, scores_rmd_out):.2f}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maha: CIFAR100-->CIFAR10 AUROC: 78.50\n",
      "Relative Maha: CIFAR100-->CIFAR10 AUROC: 78.36\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected results\n",
    "```\n",
    "Maha: CIFAR100-->CIFAR10 AUROC: 69.43\n",
    "Relative Maha: CIFAR100-->CIFAR10 AUROC: 64.32\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part VIII. K-means clusters combined with Mahalanobis distance\n",
    "\n",
    "The paper [SSD: A Unified Framework for Self-Supervised Outlier Detection](https://arxiv.org/abs/2103.12051) has proposed another unsupervised method for OOD detection. Instead of using the (real or pseudo) labels as class-wise means, we will now use the obtained clusters as found be kmeans. In more detail:\n",
    "\n",
    "- Find k=10,50,100 clusters using Kmeans on the in-distribution training data (you can use the sklearn KMeans implementation).\n",
    "- Get the cluster centers.\n",
    "- Use them as class-wise means for the mahalanobis distance classifier."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-27T00:02:24.506147Z",
     "start_time": "2025-06-27T00:00:05.652110Z"
    }
   },
   "source": [
    "# The code below is provided based on our implementation. Optional to use!\n",
    "# load features - modify names if you use different names\n",
    "indist_train = torch.load('features/cifar100_train_feats.pt')\n",
    "indist_test = torch.load('features/cifar100_test_feats.pt')\n",
    "ood_test = torch.load('features/cifar10_test_feats.pt')\n",
    "results_md = []\n",
    "results_rmd = []\n",
    "for N in [10, 50, 100]:\n",
    "    ### START CODE HERE ### (≈ 7 lines of code)\n",
    "    kmeans = KMeans(n_clusters=N, random_state=42).fit(indist_train.cpu().numpy())\n",
    "    train_cluster_labels = torch.from_numpy(kmeans.labels_).to(indist_train.device)\n",
    "    scores_md_in, scores_md_out = OOD_classifier_maha(indist_train, train_cluster_labels, indist_test, ood_test, num_classes, relative=False)\n",
    "    scores_rmd_in, scores_rmd_out = OOD_classifier_maha(indist_train, train_cluster_labels, indist_test, ood_test, num_classes, relative=True)\n",
    "    auroc_md = auroc_score(scores_md_in, scores_md_out)\n",
    "    auroc_rmd = auroc_score(scores_rmd_in, scores_rmd_out)\n",
    "    ### END CODE HERE ###\n",
    "    print(f'Kmeans (k={N}) + MD: CIFAR100-->CIFAR10 AUROC: {auroc_md:.2f}')\n",
    "    print(f'Kmeans (k={N}) + RMD: CIFAR100-->CIFAR10 AUROC: {auroc_rmd:.2f}')\n",
    "    print(\"-\" * 100)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans (k=10) + MD: CIFAR100-->CIFAR10 AUROC: 76.44\n",
      "Kmeans (k=10) + RMD: CIFAR100-->CIFAR10 AUROC: 71.02\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Kmeans (k=50) + MD: CIFAR100-->CIFAR10 AUROC: 59.42\n",
      "Kmeans (k=50) + RMD: CIFAR100-->CIFAR10 AUROC: 56.48\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Kmeans (k=100) + MD: CIFAR100-->CIFAR10 AUROC: 47.35\n",
      "Kmeans (k=100) + RMD: CIFAR100-->CIFAR10 AUROC: 46.53\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected results\n",
    "Can differ based on KMeans performance.\n",
    "\n",
    "```\n",
    "Kmeans (k=10) + MD: CIFAR100-->CIFAR10 AUROC: 68.82\n",
    "Kmeans (k=10) + RMD: CIFAR100-->CIFAR10 AUROC: 50.41\n",
    "----------------------------------------------------------------------------------------------------\n",
    "Kmeans (k=50) + MD: CIFAR100-->CIFAR10 AUROC: 68.18\n",
    "Kmeans (k=50) + RMD: CIFAR100-->CIFAR10 AUROC: 46.16\n",
    "----------------------------------------------------------------------------------------------------\n",
    "Kmeans (k=100) + MD: CIFAR100-->CIFAR10 AUROC: 67.02\n",
    "Kmeans (k=100) + RMD: CIFAR100-->CIFAR10 AUROC: 46.65\n",
    "----------------------------------------------------------------------------------------------------\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "[Exercise 3 solution] - Self-distillation on CIFAR100 .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc5fcf396fe0abd4fa852aee332a0572494dcaf5776820055c87d9b84157f362"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "15451b89dea54127867d368514cfea78": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d335008de124ef1890de5087a8254f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_15451b89dea54127867d368514cfea78",
      "placeholder": "​",
      "style": "IPY_MODEL_a1a228b8e820488aa9d7a39326832b43",
      "value": ""
     }
    },
    "5c84f8f441eb4b65a2947a8b0076b78c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "947edc38a98549e79c0906847f20560b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9bb9f2adc1f4480ba6ae6b390eda4521": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0db04ae470a41c098cb9b59a67e899b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1a228b8e820488aa9d7a39326832b43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a83800177edb46bdb5789bd0507c55b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9bb9f2adc1f4480ba6ae6b390eda4521",
      "max": 169001437,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d6ea4f8b6c304a279b56769a81897f5f",
      "value": 169001437
     }
    },
    "aca8ccf946814899be23fad49d4c4ecb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0db04ae470a41c098cb9b59a67e899b",
      "placeholder": "​",
      "style": "IPY_MODEL_947edc38a98549e79c0906847f20560b",
      "value": " 169001984/? [00:10&lt;00:00, 16869945.76it/s]"
     }
    },
    "ae807dfd8be647d29b923f286caa47c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4d335008de124ef1890de5087a8254f8",
       "IPY_MODEL_a83800177edb46bdb5789bd0507c55b1",
       "IPY_MODEL_aca8ccf946814899be23fad49d4c4ecb"
      ],
      "layout": "IPY_MODEL_5c84f8f441eb4b65a2947a8b0076b78c"
     }
    },
    "d6ea4f8b6c304a279b56769a81897f5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
